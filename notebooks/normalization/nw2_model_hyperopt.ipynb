{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Max\\anaconda3\\envs\\soiltwin\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Max\\anaconda3\\envs\\soiltwin\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\Max\\anaconda3\\envs\\soiltwin\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\Max\\anaconda3\\envs\\soiltwin\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from pytz import utc\n",
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# mlflow imports\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# xgboost imports\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# hyperopt imports\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "\n",
    "# oma_tracking imports\n",
    "from oma_tracking.data.utils import read_simulations_csv_files\n",
    "from oma_tracking.data import make_dataset\n",
    "from oma_tracking.data.preprocessing import AngleTransformer, sin_cos_angle_inputs\n",
    "import oma_tracking.models.mlflow_functions as mlflow_f\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =  datetime.datetime(2022,11,1,tzinfo=utc)\n",
    "stop  = datetime.datetime(2023,3,1,tzinfo=utc)\n",
    "location = 'nw2d01'\n",
    "name_location = 'NW2_D01'\n",
    "\n",
    "# Data Paths\n",
    "data_path = \"../../data/nw2/raw/nw2d01_\" + start.strftime(\"%Y%m%d\") + \"_\" + stop.strftime(\"%Y%m%d\") + \".parquet\"\n",
    "mvbc_path = \"../../data/nw2/mvbc_data.parquet\"\n",
    "tracked_frequencies_path = \"../../data/nw2/tracked_modes/\" + location + \".parquet\"\n",
    "simulations_data_path = \"../../data/nw2/simulations/\" + location + \"/\"\n",
    "\n",
    "# Get all the data\n",
    "data = pd.read_parquet(data_path)\n",
    "mvbc_data = pd.read_parquet(mvbc_path)\n",
    "tracked_frequencies = pd.read_parquet(tracked_frequencies_path)\n",
    "simulation_data = read_simulations_csv_files(simulations_data_path + \"eigen_frequencies/\")\n",
    "simulation_shifts = read_simulations_csv_files(simulations_data_path + \"mean_shifts/\")\n",
    "simulation_errors = pd.read_csv(simulations_data_path + \"errors/Errors_No_scour.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\max\\documents\\owi_lab\\code\\packages\\oma_tracking\\oma_tracking\\models\\mlflow_functions.py:124: UserWarning: Mlflow_tracking_uri passed without checking checking username for ':' and '@' symbols. Manually control the uri!\n",
      "  warnings.warn(\n",
      "2023/03/30 18:10:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n",
      "(psycopg2.OperationalError) connection to server at \"10.0.0.139\", port 5432 failed: Connection timed out (0x0000274C/10060)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
      "Operation will be retried in 0.1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow experiment set to: NW2_scour\n"
     ]
    }
   ],
   "source": [
    "AZURE_STORAGE_ACCESS_KEY = os.getenv('AZURE_STORAGE_ACCESS_KEY')\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "artifact_root = 'wasbs://test@mlflowstoragev1.blob.core.windows.net'\n",
    "mlflow_ui_string = mlflow_f.create_mlflow_ui(MLFLOW_TRACKING_URI, artifact_root)\n",
    "database_url = 'http://127.0.0.1:5000'\n",
    "mlflow_f.connect_mlflow_ui(mlflow_ui_string, database_url)\n",
    "\n",
    "experiment_name = 'NW2_scour'\n",
    "experiment = mlflow_f.run_mlflow_experiment(experiment_name = experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_inputs = make_dataset.get_weather_subset(mvbc_data)\n",
    "scada_inputs = make_dataset.get_scada_subset(data)\n",
    "\n",
    "inputs = pd.concat([\n",
    "            weather_inputs,\n",
    "            scada_inputs\n",
    "        ],axis=1)\n",
    "\n",
    "prediction_params = tracked_frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_folder = \"../../data/nw2/model_hyperopt/\" + location + \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [40:27<00:00, 12.14s/trial, best loss: -0.28305908969794374]\n",
      "{'colsample_bytree': 0.5249699361570449, 'learning_rate': 0.028744322686588067, 'max_depth': 6.0, 'n_estimators': 275.0}\n",
      "100%|██████████| 200/200 [47:29<00:00, 14.25s/trial, best loss: -0.5340336897612492]\n",
      "{'colsample_bytree': 0.48684749114948356, 'learning_rate': 0.021045327649482887, 'max_depth': 8.0, 'n_estimators': 441.0}\n",
      "100%|██████████| 200/200 [39:31<00:00, 11.86s/trial, best loss: -0.365799277323665]  \n",
      "{'colsample_bytree': 0.45826301668701863, 'learning_rate': 0.020502094520931994, 'max_depth': 10.0, 'n_estimators': 456.0}\n",
      "100%|██████████| 200/200 [38:01<00:00, 11.41s/trial, best loss: -0.5417091611145215]\n",
      "{'colsample_bytree': 0.5257177720137283, 'learning_rate': 0.06075855570180731, 'max_depth': 6.0, 'n_estimators': 242.0}\n"
     ]
    }
   ],
   "source": [
    "seed = 2\n",
    "def objective_xgb(space):\n",
    "    model = Pipeline(\n",
    "            steps=[\n",
    "                ('preprocessing_angles', AngleTransformer(angles = ['winddirection', 'yaw'])),\n",
    "                ('regressor', XGBRegressor(\n",
    "                                 n_estimators = space['n_estimators'],\n",
    "                                 max_depth = space['max_depth'],\n",
    "                                 learning_rate = space['learning_rate'],\n",
    "                                 colsample_bytree = space['colsample_bytree'],\n",
    "                                 )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    score = cross_val_score(model,  X_train, y_train, cv=5, scoring='r2').mean()\n",
    "    # We aim to maximize r2 score, therefore we return it as a negative value\n",
    "    return {'loss': -score, 'status': STATUS_OK }\n",
    "def optimize_xgb(trial):\n",
    "    space = {\n",
    "        'n_estimators':hp.uniformint('n_estimators',10,500),\n",
    "        'max_depth':hp.uniformint('max_depth',3,20),\n",
    "        'learning_rate':hp.uniform('learning_rate',0.01,0.5),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree',0.1, 1),\n",
    "    }\n",
    "    best = \\\n",
    "        fmin(\n",
    "            fn = objective_xgb,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trial,\n",
    "            max_evals = 200,\n",
    "            rstate = np.random.default_rng(seed)\n",
    "            )\n",
    "    return best\n",
    "\n",
    "XGB_optimizations = {}\n",
    "for mode in prediction_params.columns:\n",
    "    y = prediction_params[mode].dropna()\n",
    "    X = inputs.loc[y.index].dropna()\n",
    "    y = y.loc[X.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n",
    "    trial2=Trials()\n",
    "    XGB_optimizations[mode]=optimize_xgb(trial2)\n",
    "    print(XGB_optimizations[mode])\n",
    "pd.DataFrame(XGB_optimizations).to_csv(hyperopt_folder + \"xgb_optimizations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x1e8bb8d6640>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [4:31:12<00:00, 162.73s/trial, best loss: -0.2776385007194554]  \n",
      "{'max_depth': 11.0, 'min_samples_leaf': 5.0, 'min_samples_split': 3.0, 'n_estimators': 301.0}\n",
      "100%|██████████| 100/100 [3:46:24<00:00, 135.85s/trial, best loss: -0.5375054044244332] \n",
      "{'max_depth': 12.0, 'min_samples_leaf': 5.0, 'min_samples_split': 5.0, 'n_estimators': 458.0}\n",
      "100%|██████████| 100/100 [2:33:48<00:00, 92.29s/trial, best loss: -0.37939700141767807] \n",
      "{'max_depth': 18.0, 'min_samples_leaf': 5.0, 'min_samples_split': 2.0, 'n_estimators': 276.0}\n",
      "100%|██████████| 100/100 [2:54:05<00:00, 104.45s/trial, best loss: -0.5311810475298762] \n",
      "{'max_depth': 11.0, 'min_samples_leaf': 5.0, 'min_samples_split': 2.0, 'n_estimators': 294.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "seed = 2\n",
    "\n",
    "def objective_rf(space):\n",
    "    model = Pipeline(\n",
    "            steps=[\n",
    "                ('preprocessing_angles', AngleTransformer(angles = ['winddirection', 'yaw'])),\n",
    "                ('regressor', RandomForestRegressor(\n",
    "                                 n_estimators = space['n_estimators'],\n",
    "                                 max_depth = space['max_depth'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split']\n",
    "                                 )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    score = cross_val_score(model,  X_train, y_train, cv=5, scoring='r2').mean()\n",
    "    # We aim to maximize r2 score, therefore we return it as a negative value\n",
    "    return {'loss': -score, 'status': STATUS_OK }\n",
    "\n",
    "def optimize_rf(trial):\n",
    "    space = {\n",
    "        'n_estimators': hp.uniformint('n_estimators',10,500),\n",
    "        'max_depth': hp.uniformint('max_depth',3,20),\n",
    "        'min_samples_leaf': hp.uniformint('min_samples_leaf',1,5),\n",
    "        'min_samples_split': hp.uniformint('min_samples_split',2,6)\n",
    "    }\n",
    "    best = \\\n",
    "        fmin(\n",
    "            fn = objective_rf,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trial,\n",
    "            max_evals = 100,\n",
    "            rstate = np.random.default_rng(seed)\n",
    "            )\n",
    "    return best\n",
    "\n",
    "RF_optimizations = {}\n",
    "for mode in prediction_params.columns:\n",
    "    y = prediction_params[mode].dropna()\n",
    "    X = inputs.loc[y.index].dropna()\n",
    "    y = y.loc[X.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    trial=Trials()\n",
    "    RF_optimizations[mode]=optimize_rf(trial)\n",
    "    print(RF_optimizations[mode])\n",
    "pd.DataFrame(RF_optimizations).to_csv(hyperopt_folder + \"rf_optimizations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x1e8bd8cf7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soiltwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59700db8913d1834820c267fdc13540c1f9ab675c387b03ed985ffb4bb0979f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
